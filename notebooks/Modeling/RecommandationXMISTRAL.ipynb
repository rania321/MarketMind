{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Test Model"
      ],
      "metadata": {
        "id": "xWwv3BBvKSay"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "!pip install unsloth\n",
        "# Also get the latest nightly Unsloth!\n",
        "!pip uninstall unsloth -y && pip install --upgrade --no-cache-dir --no-deps git+https://github.com/unslothai/unsloth.git"
      ],
      "metadata": {
        "id": "joEBBAgpUbur"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Definition of an Alpaca Prompt Model for Generating Contextual Responses"
      ],
      "metadata": {
        "id": "-Frr84d-K2wn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "alpaca_prompt = \"\"\"Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
        "\n",
        "### Instruction:\n",
        "{}\n",
        "\n",
        "### Input:\n",
        "{}\n",
        "\n",
        "### Response:\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "YDBWAbK6UGVz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Configuration of Model Parameters for Efficient Inference"
      ],
      "metadata": {
        "id": "A8UDSEc9LCdz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "max_seq_length = 2048 # Choose any! We auto support RoPE Scaling internally!\n",
        "dtype = None # None for auto detection. Float16 for Tesla T4, V100, Bfloat16 for Ampere+\n",
        "load_in_4bit = True"
      ],
      "metadata": {
        "id": "tuU1pkk4Vk7c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Loading and Configuring Pre-trained Language Model for Inference with FastLanguageModel"
      ],
      "metadata": {
        "id": "FeMkK-xFLN9I"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Tr2lRE1XTQs_",
        "outputId": "d87757d6-21a9-4e66-bbce-9a7a34eec5e8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "==((====))==  Unsloth 2025.3.19: Fast Llama patching. Transformers: 4.50.0.\n",
            "   \\\\   /|    Tesla T4. Num GPUs = 1. Max memory: 14.741 GB. Platform: Linux.\n",
            "O^O/ \\_/ \\    Torch: 2.6.0+cu124. CUDA: 7.5. CUDA Toolkit: 12.4. Triton: 3.2.0\n",
            "\\        /    Bfloat16 = FALSE. FA [Xformers = 0.0.29.post3. FA2 = False]\n",
            " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
            "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"
          ]
        }
      ],
      "source": [
        "if True:\n",
        "    from unsloth import FastLanguageModel\n",
        "    model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "        model_name = \"Nourhen2001/SummarizeReviews_model_v2\", # YOUR MODEL YOU USED FOR TRAINING\n",
        "        max_seq_length = max_seq_length,\n",
        "        dtype = dtype,\n",
        "        load_in_4bit = load_in_4bit,\n",
        "    )\n",
        "    FastLanguageModel.for_inference(model) # Enable native 2x faster inference\n",
        "\n",
        "# alpaca_prompt = You MUST copy from above!\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Generating Customer Review Summaries Using Pre-trained Language Model with Tokenizer and TextStreamer"
      ],
      "metadata": {
        "id": "0efE3mElLmui"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "inputs = tokenizer(\n",
        "[\n",
        "    alpaca_prompt.format(\n",
        "        \"Summarize customer reviews.\",  # instruction\n",
        "        \"Product: Bluetooth Headphones - SoundBeats | Positive: 'Excellent sound quality at a great price.', 'Fast delivery and helpful customer support during setup.', 'Great value for money.' | Negative: 'Could be cheaper for the build quality.', 'Delivery was delayed a few days, but customer service was responsive.\",  # input\n",
        "        \"\" # output - leave this blank for generation!\n",
        "    )\n",
        "], return_tensors = \"pt\").to(\"cuda\")\n",
        "\n",
        "from transformers import TextStreamer\n",
        "text_streamer = TextStreamer(tokenizer)\n",
        "_ = model.generate(**inputs, streamer = text_streamer, max_new_tokens = 1000)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6GK8Tn_DVnao",
        "outputId": "796c16c8-f1d0-42dd-9dd3-78ee89f639d6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<|begin_of_text|>Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
            "\n",
            "### Instruction:\n",
            "Summarize customer reviews.\n",
            "\n",
            "### Input:\n",
            "Product: Bluetooth Headphones - SoundBeats | Positive: 'Excellent sound quality at a great price.', 'Fast delivery and helpful customer support during setup.', 'Great value for money.' | Negative: 'Could be cheaper for the build quality.', 'Delivery was delayed a few days, but customer service was responsive.\n",
            "\n",
            "### Response:\n",
            "SoundBeats Bluetooth headphones offer great value for money with excellent sound quality, fast delivery, and good customer support. Some customers feel the build quality could be better, and delivery was delayed for a few.<|eot_id|>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "1SOoaDRA22W6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**THE RECOMMANDATION MODEL**\n"
      ],
      "metadata": {
        "id": "3RF1zI3JyiCD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from unsloth import FastLanguageModel\n"
      ],
      "metadata": {
        "id": "RYHKN6Nz6gVz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "# Load a model for recommendations (could be same/different from your summarization model)\n",
        "rec_model, rec_tokenizer = FastLanguageModel.from_pretrained(\n",
        "    model_name = \"unsloth/mistral-7b-instruct-v0.2\",  # Small & fast for recommendations\n",
        "    max_seq_length = 2048,\n",
        "    dtype = None,\n",
        "    load_in_4bit = True,\n",
        ")\n",
        "\n",
        "\n",
        "FastLanguageModel.for_inference(rec_model)"
      ],
      "metadata": {
        "id": "7iGgxdZ15AsU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define a function to generate recommendations\n",
        "def generate_recommendations(summary):\n",
        "    recommendation_prompt = \"\"\"I will give you customer feedback and you respond with 3 business recommendations.\n",
        "\n",
        "Example 1:\n",
        "Feedback: \"Great battery life but uncomfortable fit\"\n",
        "Recommendations:\n",
        "1. Redesign ear cushions for better comfort while maintaining battery\n",
        "2. Offer multiple ear tip sizes in the package\n",
        "3. Market the battery life as a key differentiator\n",
        "\n",
        "Example 2:\n",
        "Feedback: \"Fast shipping but product arrived damaged\"\n",
        "Recommendations:\n",
        "1. Improve packaging materials to prevent damage\n",
        "2. Implement quality checks before shipping\n",
        "3. Create a faster replacement policy for damaged items\n",
        "\n",
        "Now generate recommendations for:\n",
        "{}\"\"\".format(summary)  # Now summary is correctly passed\n",
        "\n",
        "    inputs = rec_tokenizer([recommendation_prompt], return_tensors=\"pt\").to(\"cuda\")\n",
        "    outputs = rec_model.generate(**inputs, max_new_tokens=256)\n",
        "\n",
        "    return rec_tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "\n",
        "# Example usage\n",
        "summary = \"SoundBeats headphones offer excellent sound quality at a reasonable price, with fast delivery and helpful customer support. However, some feel the build quality could be better for the price, and delivery was delayed for a few days.\"\n",
        "recommendations = generate_recommendations(summary)\n",
        "print(recommendations)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NvzL5KnQ7HHI",
        "outputId": "e814aab6-23ce-43af-d07b-a2736552cb16"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "I will give you customer feedback and you respond with 3 business recommendations.\n",
            "\n",
            "Example 1:\n",
            "Feedback: \"Great battery life but uncomfortable fit\"\n",
            "Recommendations:\n",
            "1. Redesign ear cushions for better comfort while maintaining battery\n",
            "2. Offer multiple ear tip sizes in the package\n",
            "3. Market the battery life as a key differentiator\n",
            "\n",
            "Example 2:\n",
            "Feedback: \"Fast shipping but product arrived damaged\"\n",
            "Recommendations:\n",
            "1. Improve packaging materials to prevent damage\n",
            "2. Implement quality checks before shipping\n",
            "3. Create a faster replacement policy for damaged items\n",
            "\n",
            "Now generate recommendations for:\n",
            "SoundBeats headphones offer excellent sound quality at a reasonable price, with fast delivery and helpful customer support. However, some feel the build quality could be better for the price, and delivery was delayed for a few days.\n",
            "\n",
            "Recommendations:\n",
            "1. Upgrade materials used in the headphone construction to improve perceived value\n",
            "2. Implement more robust shipping methods to ensure timely delivery\n",
            "3. Offer extended warranties or repair services to address build quality concerns.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1: Get summary from your existing model\n",
        "inputs = tokenizer(\n",
        "    [\n",
        "        alpaca_prompt.format(\n",
        "            \"Summarize customer reviews.\",\n",
        "            \"Product: Bluetooth Headphones - SoundBeats | Positive: 'Excellent sound quality at a great price.', 'Fast delivery and helpful customer support during setup.', 'Great value for money.' | Negative: 'Could be cheaper for the build quality.', 'Delivery was delayed a few days, but customer service was responsive.\",\n",
        "            \"\"\n",
        "        )\n",
        "    ], return_tensors=\"pt\").to(\"cuda\")\n",
        "\n",
        "summary = model.generate(**inputs, max_new_tokens=1000)\n",
        "summary_text = tokenizer.decode(summary[0], skip_special_tokens=True)\n",
        "\n",
        "# Extract clean summary text\n",
        "clean_summary = summary_text.split(\"### Response:\")[-1].strip()\n",
        "\n",
        "# Step 2: Generate recommendations\n",
        "print(\"Summary from Model 1:\\n\", clean_summary)\n",
        "print(\"\\nRecommendations from Model 2:\\n\", generate_recommendations(clean_summary))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rkuasH_k7ACM",
        "outputId": "276e9a69-8e3e-41b6-93b1-b7d097530ece"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Summary from Model 1:\n",
            " SoundBeats Bluetooth headphones are praised for their sound quality and value, though some feel the build quality and delivery could be improved.\n",
            "\n",
            "Recommendations from Model 2:\n",
            " I will give you customer feedback and you respond with 3 business recommendations.\n",
            "\n",
            "Example 1:\n",
            "Feedback: \"Great battery life but uncomfortable fit\"\n",
            "Recommendations:\n",
            "1. Redesign ear cushions for better comfort while maintaining battery\n",
            "2. Offer multiple ear tip sizes in the package\n",
            "3. Market the battery life as a key differentiator\n",
            "\n",
            "Example 2:\n",
            "Feedback: \"Fast shipping but product arrived damaged\"\n",
            "Recommendations:\n",
            "1. Improve packaging materials to prevent damage\n",
            "2. Implement quality checks before shipping\n",
            "3. Create a faster replacement policy for damaged items\n",
            "\n",
            "Now generate recommendations for:\n",
            "SoundBeats Bluetooth headphones are praised for their sound quality and value, though some feel the build quality and delivery could be improved.\n",
            "\n",
            "Recommendations:\n",
            "1. Upgrade materials used in the headphone construction to enhance durability and perceived value.\n",
            "2. Implement a more reliable and faster shipping method to meet customer expectations.\n",
            "3. Offer extended warranties or repair services to address concerns about build quality.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "!pip install unsloth\n",
        "!pip uninstall unsloth -y && pip install --upgrade --no-cache-dir --no-deps git+https://github.com/unslothai/unsloth.git\n",
        "\n",
        "# Load the first model (Summarization)\n",
        "from unsloth import FastLanguageModel\n",
        "model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "    model_name=\"Nourhen2001/SummarizeReviews_model_v2\",\n",
        "    max_seq_length=1024,\n",
        "    dtype=None,\n",
        "    load_in_4bit=True,  # Utiliser 4-bit\n",
        "    device_map=\"auto\",  # Optimiser l'utilisation du GPU et CPU\n",
        "    llm_int8_enable_fp32_cpu_offload=True  # Activer l'offloading contrôlé\n",
        ")\n",
        "\n",
        "FastLanguageModel.for_inference(model)\n",
        "\n",
        "# Load the second model (Recommendations)\n",
        "rec_model, rec_tokenizer = FastLanguageModel.from_pretrained(\n",
        "    model_name=\"unsloth/mistral-7b-instruct-v0.2\",\n",
        "    max_seq_length=2048,\n",
        "    dtype=None,\n",
        "    load_in_4bit=True,\n",
        ")\n",
        "FastLanguageModel.for_inference(rec_model)\n",
        "\n",
        "# Define prompt formats\n",
        "alpaca_prompt = \"\"\"Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
        "\n",
        "### Instruction:\n",
        "{}\n",
        "\n",
        "### Input:\n",
        "{}\n",
        "\n",
        "### Response:\n",
        "\"\"\"\n",
        "\n",
        "recommendation_prompt = \"\"\"I will give you customer feedback and you respond with 3 business recommendations.\n",
        "\n",
        "Example 1:\n",
        "Feedback: \"Great battery life but uncomfortable fit\"\n",
        "Recommendations:\n",
        "1. Redesign ear cushions for better comfort while maintaining battery life.\n",
        "2. Offer multiple ear tip sizes in the package.\n",
        "3. Market the battery life as a key differentiator.\n",
        "\n",
        "Example 2:\n",
        "Feedback: \"Fast shipping but product arrived damaged\"\n",
        "Recommendations:\n",
        "1. Improve packaging materials to prevent damage.\n",
        "2. Implement quality checks before shipping.\n",
        "3. Create a faster replacement policy for damaged items.\n",
        "\n",
        "Now generate recommendations for:\n",
        "{}\"\"\"\n",
        "\n",
        "def generate_summary(reviews):\n",
        "    inputs = tokenizer(\n",
        "        [alpaca_prompt.format(\"Summarize customer reviews.\", reviews, \"\")],\n",
        "        return_tensors=\"pt\"\n",
        "    ).to(\"cuda\")\n",
        "\n",
        "    summary_output = model.generate(**inputs, max_new_tokens=1000)\n",
        "    summary_text = tokenizer.decode(summary_output[0], skip_special_tokens=True)\n",
        "    return summary_text.split(\"### Response:\")[-1].strip()  # Extract clean summary\n",
        "\n",
        "def generate_recommendations(summary):\n",
        "    inputs = rec_tokenizer(\n",
        "        [recommendation_prompt.format(summary)], return_tensors=\"pt\"\n",
        "    ).to(\"cuda\")\n",
        "\n",
        "    outputs = rec_model.generate(**inputs, max_new_tokens=256)\n",
        "    return rec_tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "\n",
        "# Sample input\n",
        "customer_reviews = \"\"\"Product: Bluetooth Headphones - SoundBeats | Positive: 'Excellent sound quality at a great price.',\n",
        "'Fast delivery and helpful customer support during setup.', 'Great value for money.' |\n",
        "Negative: 'Could be cheaper for the build quality.', 'Delivery was delayed a few days, but customer service was responsive.'\"\"\"\n",
        "\n",
        "# Step 1: Generate summary\n",
        "summary_result = generate_summary(customer_reviews)\n",
        "print(\"Summary from Model 1:\\n\", summary_result)\n",
        "\n",
        "# Step 2: Generate recommendations\n",
        "recommendations_result = generate_recommendations(summary_result)\n",
        "print(\"\\nRecommendations from Model 2:\\n\", recommendations_result)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 408
        },
        "id": "IHF7D8Da7amq",
        "outputId": "53e4ba53-6ca4-4105-d5d4-15b4f0b811eb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "LlamaForCausalLM.__init__() got an unexpected keyword argument 'llm_int8_enable_fp32_cpu_offload'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-34-b359ea460a9e>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# Load the first model (Summarization)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0munsloth\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mFastLanguageModel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m model, tokenizer = FastLanguageModel.from_pretrained(\n\u001b[0m\u001b[1;32m      7\u001b[0m     \u001b[0mmodel_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"Nourhen2001/SummarizeReviews_model_v2\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0mmax_seq_length\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1024\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/unsloth/models/loader.py\u001b[0m in \u001b[0;36mfrom_pretrained\u001b[0;34m(model_name, max_seq_length, dtype, load_in_4bit, load_in_8bit, full_finetuning, token, device_map, rope_scaling, fix_tokenizer, trust_remote_code, use_gradient_checkpointing, resize_model_vocab, revision, use_exact_model_name, fast_inference, gpu_memory_utilization, float8_kv_cache, random_state, max_lora_rank, disable_log_stats, *args, **kwargs)\u001b[0m\n\u001b[1;32m    361\u001b[0m         \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    362\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 363\u001b[0;31m         model, tokenizer = dispatch_model.from_pretrained(\n\u001b[0m\u001b[1;32m    364\u001b[0m             \u001b[0mmodel_name\u001b[0m        \u001b[0;34m=\u001b[0m \u001b[0mmodel_name\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    365\u001b[0m             \u001b[0mmax_seq_length\u001b[0m    \u001b[0;34m=\u001b[0m \u001b[0mmax_seq_length\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/unsloth/models/llama.py\u001b[0m in \u001b[0;36mfrom_pretrained\u001b[0;34m(model_name, max_seq_length, dtype, load_in_4bit, token, device_map, rope_scaling, fix_tokenizer, model_patcher, tokenizer_name, trust_remote_code, fast_inference, gpu_memory_utilization, float8_kv_cache, random_state, max_lora_rank, disable_log_stats, **kwargs)\u001b[0m\n\u001b[1;32m   1778\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1779\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mfast_inference\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1780\u001b[0;31m             model = AutoModelForCausalLM.from_pretrained(\n\u001b[0m\u001b[1;32m   1781\u001b[0m                 \u001b[0mmodel_name\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1782\u001b[0m                 \u001b[0mdevice_map\u001b[0m              \u001b[0;34m=\u001b[0m \u001b[0mdevice_map\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/models/auto/auto_factory.py\u001b[0m in \u001b[0;36mfrom_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m    571\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_model_mapping\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    572\u001b[0m             \u001b[0mmodel_class\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_get_model_class\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_model_mapping\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 573\u001b[0;31m             return model_class.from_pretrained(\n\u001b[0m\u001b[1;32m    574\u001b[0m                 \u001b[0mpretrained_model_name_or_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mmodel_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mhub_kwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    575\u001b[0m             )\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/modeling_utils.py\u001b[0m in \u001b[0;36m_wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    270\u001b[0m         \u001b[0mold_dtype\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_default_dtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    271\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 272\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    273\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    274\u001b[0m             \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_default_dtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mold_dtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/modeling_utils.py\u001b[0m in \u001b[0;36mfrom_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, config, cache_dir, ignore_mismatched_sizes, force_download, local_files_only, token, revision, use_safetensors, weights_only, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m   4399\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mContextManagers\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_init_context\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4400\u001b[0m             \u001b[0;31m# Let's make sure we don't run the init function of buffer modules\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4401\u001b[0;31m             \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mmodel_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mmodel_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4402\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4403\u001b[0m         \u001b[0;31m# Make sure to tie the weights correctly\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: LlamaForCausalLM.__init__() got an unexpected keyword argument 'llm_int8_enable_fp32_cpu_offload'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!nvidia-smi\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S2SdZsiz8rbz",
        "outputId": "85633f98-23ce-4d73-aea1-bb5d3adcec3c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Thu Mar 27 23:33:44 2025       \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 550.54.15              Driver Version: 550.54.15      CUDA Version: 12.4     |\n",
            "|-----------------------------------------+------------------------+----------------------+\n",
            "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
            "|                                         |                        |               MIG M. |\n",
            "|=========================================+========================+======================|\n",
            "|   0  Tesla T4                       Off |   00000000:00:04.0 Off |                    0 |\n",
            "| N/A   73C    P0             33W /   70W |   11174MiB /  15360MiB |      0%      Default |\n",
            "|                                         |                        |                  N/A |\n",
            "+-----------------------------------------+------------------------+----------------------+\n",
            "                                                                                         \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| Processes:                                                                              |\n",
            "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
            "|        ID   ID                                                               Usage      |\n",
            "|=========================================================================================|\n",
            "+-----------------------------------------------------------------------------------------+\n"
          ]
        }
      ]
    }
  ]
}